import requests
from bs4 import BeautifulSoup
import time
from urllib.parse import urljoin, parse_qs, urlparse
import re
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from utils.utils import save_to_json, save_to_excel


def get_all_departments_selenium(base_urls):
    """Seleniumì„ ì‚¬ìš©í•˜ì—¬ ë™ì ìœ¼ë¡œ ë¡œë“œë˜ëŠ” ëª¨ë“  ë¶€ì„œ ëª©ë¡ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    all_departments = []
    print("ğŸ¯ 1ë‹¨ê³„: Seleniumìœ¼ë¡œ ì „ì²´ ë¶€ì„œ ëª©ë¡ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    try:
        options = Options()
        options.add_argument("--headless")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36")
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
    except Exception as e:
        print(f"âŒ Selenium ë“œë¼ì´ë²„ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

    try:
        for category, url in base_urls.items():
            print(f"   - [{category}] í˜ì´ì§€ ì ‘ì† ë° ë¶„ì„ ì¤‘...")
            driver.get(url)
            time.sleep(2)
            
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            dept_links = soup.select("a.x_tag")
            
            count = 0
            for link in dept_links:
                href = link.get('href')
                if not href or 'javascript:' in href: continue
                full_url = urljoin(url, href)
                dept_name = link.get('title', '').strip() or link.get_text(strip=True).replace('# ', '')
                if dept_name and dept_name != 'ì „ì²´':
                    all_departments.append({'category': category, 'name': dept_name, 'url': full_url})
                    count += 1
            print(f"     -> {count}ê°œ ë¶€ì„œ ìˆ˜ì§‘ ì™„ë£Œ.")
    except Exception as e:
        print(f"   - í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        driver.quit()
        print("\nâœ… Selenium ë“œë¼ì´ë²„ ì¢…ë£Œ.")
            
    print(f"\nâœ… 1ë‹¨ê³„ ì™„ë£Œ: ì´ {len(all_departments)}ê°œì˜ ë¶€ì„œ ë§í¬(ì¤‘ë³µ í¬í•¨)ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    return all_departments

def fetch_doctors_from_department(department, headers):
    """ì£¼ì–´ì§„ ë¶€ì„œ í˜ì´ì§€ì—ì„œ ëª¨ë“  ì˜ë£Œì§„ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    dept_name = department['name']
    category = department['category']
    url = department['url']
    
    parsed_url = urlparse(url)
    query_params = parse_qs(parsed_url.query)
    dept_no = query_params.get('deptNo', [None])[0]

    doctors = []
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        doctor_items = soup.select("ul.c_doc_list > li.doc_blk")
        for item in doctor_items:
            name_tag = item.select_one("p.tit span.t")
            name = name_tag.get_text(strip=True) if name_tag else "ì´ë¦„ ì •ë³´ ì—†ìŒ"
            
            specialty_tag = item.select_one("dl.txt dd.link")
            specialty = specialty_tag.get_text(strip=True) if specialty_tag else "ì „ë¬¸ë¶„ì•¼ ì •ë³´ ì—†ìŒ"
            
            prof_no = "ID ì—†ìŒ"
            link_tag = item.select_one("div.btn_w a[href*='openDoctorView']")
            if link_tag:
                href_attr = link_tag.get('href', '')
                match = re.search(r"openDoctorView\(\s*'.*?',\s*'([^']*)'\s*\)", href_attr)
                if match:
                    prof_no = match.group(1)

            doctors.append({
                'ì†Œì†ë¶„ë¥˜': category, 'ì†Œì†ë¶€ì„œ': dept_name, 'ì´ë¦„': name,
                'ì „ë¬¸ë¶„ì•¼': specialty, 'deptNo': dept_no, 'profNo': prof_no
            })
        return doctors
    except requests.exceptions.RequestException as e:
        print(f"       - {dept_name} ì˜ë£Œì§„ ì •ë³´ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
        return []

def fetch_doctor_details(doctor, headers):
    """requestsë¡œ íŒì—… HTMLì— ìˆ¨ê²¨ì§„ í•™ë ¥/ê²½ë ¥ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    dept_no = doctor.get('deptNo')
    prof_no = doctor.get('profNo')
    
    if not dept_no or not prof_no or prof_no == "ID ì—†ìŒ":
        return {"í•™ë ¥": "ìƒì„¸ ì •ë³´ ì¡°íšŒ ë¶ˆê°€", "ê²½ë ¥": "ìƒì„¸ ì •ë³´ ì¡°íšŒ ë¶ˆê°€"}
        
    detail_url = f"https://hosp.ajoumc.or.kr/doctor/profViewPop.do?deptNo={dept_no}&profNo={prof_no}"
    details = {"í•™ë ¥": "ì •ë³´ ì—†ìŒ", "ê²½ë ¥": "ì •ë³´ ì—†ìŒ"}
    
    try:
        response = requests.get(detail_url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ìˆ¨ê²¨ì§„ mobileìš© divì—ì„œ ì •ë³´ ì¶”ì¶œ
        career_area = soup.select_one("div#careerMobArea")
        if career_area:
            sections = career_area.select("ul.detailsBox_txt_list > li")
            for section in sections:
                title_tag = section.select_one("p.tit span.t")
                if not title_tag: continue
                
                title = title_tag.get_text(strip=True)
                items = []
                for li in section.select("ul.list_basic.list_dot > li"):
                    item_text = ' '.join(li.find('span').find_all(string=True, recursive=False)).strip()
                    item_text = re.sub(r'\s+', ' ', item_text)
                    items.append(item_text)
                
                if items:
                    if 'í•™ë ¥' in title:
                        details['í•™ë ¥'] = "\n".join(items)
                    elif 'ê²½ë ¥' in title:
                        details['ê²½ë ¥'] = "\n".join(items)

    except requests.exceptions.RequestException as e:
        print(f"     [Error] ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬: {e}")
    
    return details

if __name__ == "__main__":
    base_urls = {
        "ì§„ë£Œê³¼": "https://hosp.ajoumc.or.kr/doctor/profDeptList.do",
        "ì „ë¬¸ì„¼í„°": "https://hosp.ajoumc.or.kr/doctor/profCenterList.do",
        "ì•”ì„¼í„°": "https://hosp.ajoumc.or.kr/doctor/profCancerList.do",
        "ì¹˜ê³¼ë³‘ì›": "https://hosp.ajoumc.or.kr/doctor/profDentalList.do",
        "ì „ë¬¸í´ë¦¬ë‹‰": "https://hosp.ajoumc.or.kr/doctor/profClinicList.do"
    }
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36'
    }

    json_output_file = 'ajou_doctors_with_details.json'
    excel_output_file = 'ajou_doctors_with_details.xlsx'
    
    departments = get_all_departments_selenium(base_urls)
    
    if departments:
        unique_checker_dept = {}
        for dept in departments:
            unique_key = (dept['name'], dept['url'])
            if unique_key not in unique_checker_dept:
                unique_checker_dept[unique_key] = dept
        unique_departments = list(unique_checker_dept.values())
        print(f"\nâœ… ì¤‘ë³µ ì œê±° í›„, ìµœì¢… {len(unique_departments)}ê°œì˜ ë¶€ì„œë¥¼ ëŒ€ìƒìœ¼ë¡œ 2ë‹¨ê³„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

        all_doctors = []
        print("\nğŸ¯ 2ë‹¨ê³„: ê° ë¶€ì„œë³„ ì˜ë£Œì§„ ëª©ë¡ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        for i, dept in enumerate(unique_departments):
            print(f"   - ({i+1}/{len(unique_departments)}) {dept['name']} ({dept['category']}) ì˜ë£Œì§„ ìˆ˜ì§‘ ì¤‘...")
            doctors_in_dept = fetch_doctors_from_department(dept, headers)
            if doctors_in_dept:
                print(f"     -> {len(doctors_in_dept)}ëª… ìˆ˜ì§‘ ì™„ë£Œ.")
                all_doctors.extend(doctors_in_dept)
            else:
                print("     -> ì˜ë£Œì§„ ì •ë³´ ì—†ìŒ.")
            time.sleep(0.3)
            
        print(f"\nâœ… 2ë‹¨ê³„ ì™„ë£Œ: ìˆ˜ì§‘ëœ ì˜ë£Œì§„ ì •ë³´ëŠ” ì´ {len(all_doctors)}ê±´ ì…ë‹ˆë‹¤.")
        
        unique_doctors = list({doc['profNo']: doc for doc in all_doctors if doc['profNo'] != "ID ì—†ìŒ"}.values())
        print(f"âœ… ì¤‘ë³µ ì œê±° í›„, ìµœì¢… {len(unique_doctors)}ëª…ì˜ ì˜ë£Œì§„ ì •ë³´ë¥¼ ëŒ€ìƒìœ¼ë¡œ 3ë‹¨ê³„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

        print("\nğŸ¯ 3ë‹¨ê³„: ê° ì˜ë£Œì§„ì˜ ìƒì„¸ ì •ë³´(í•™ë ¥/ê²½ë ¥) ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        final_data = []
        for i, doc in enumerate(unique_doctors):
            print(f"   - ({i+1}/{len(unique_doctors)}) {doc['ì´ë¦„']} ì˜ë£Œì§„ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
            details = fetch_doctor_details(doc, headers)
            doc['í•™ë ¥'] = details['í•™ë ¥']
            doc['ê²½ë ¥'] = details['ê²½ë ¥']
            final_data.append(doc)
            time.sleep(0.3)

        print(f"\nâœ… 3ë‹¨ê³„ ì™„ë£Œ: ìµœì¢…ì ìœ¼ë¡œ {len(final_data)}ëª…ì˜ ìƒì„¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        
        # ğŸ”½ utils.pyì˜ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ ì €ì¥

        file_name = 'ì•„ì£¼ëŒ€í•™êµë³‘ì›_ajou'

        save_to_json(final_data, file_name)
        save_to_excel(final_data, file_name)
        
    else:
        print("\nâŒ 1ë‹¨ê³„ ë¶€ì„œ ìˆ˜ì§‘ì— ì‹¤íŒ¨í•˜ì—¬ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
