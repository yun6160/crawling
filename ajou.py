# app.py

import requests
from bs4 import BeautifulSoup
import json
import time
from urllib.parse import urljoin, parse_qs, urlparse
import re
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
import pandas as pd
import os

def save_to_json(data, filename):
    """ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=4, ensure_ascii=False)
        print(f"âœ… ì„±ê³µ! ë°ì´í„°ê°€ '{filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except (IOError, TypeError) as e:
        print(f"âŒ íŒŒì¼ ì €ì¥ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")

def convert_json_to_excel(json_filename, excel_filename):
    """JSON íŒŒì¼ì„ ì½ì–´ Excel íŒŒì¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜"""
    print("\nğŸ”„ 4ë‹¨ê³„: ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ Excel íŒŒì¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤...")
    try:
        # 1. JSON íŒŒì¼ ì½ê¸°
        with open(json_filename, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if not data:
            print("âš ï¸ ë³€í™˜í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. Excel íŒŒì¼ì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return

        # 2. pandas DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame(data)
        
        # 3. DataFrameì„ Excel íŒŒì¼ë¡œ ì €ì¥
        # index=False ì˜µì…˜ì€ ì—‘ì…€ì— ë¶ˆí•„ìš”í•œ ì¸ë±ìŠ¤ ì—´ì´ ì¶”ê°€ë˜ëŠ” ê²ƒì„ ë°©ì§€í•´ ì¤Œ
        df.to_excel(excel_filename, index=False, engine='openpyxl')
        
        print(f"âœ… ì„±ê³µ! ë°ì´í„°ê°€ '{excel_filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        # ì ˆëŒ€ ê²½ë¡œë¥¼ í‘œì‹œí•˜ì—¬ ì‚¬ìš©ìê°€ íŒŒì¼ì„ ì‰½ê²Œ ì°¾ì„ ìˆ˜ ìˆë„ë¡ í•¨
        print(f"   -> ì €ì¥ ìœ„ì¹˜: {os.path.abspath(excel_filename)}")

    except FileNotFoundError:
        print(f"âŒ ì—ëŸ¬: JSON íŒŒì¼ '{json_filename}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ Excel ë³€í™˜ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")


def get_all_departments_selenium(base_urls):
    """Seleniumì„ ì‚¬ìš©í•˜ì—¬ ë™ì ìœ¼ë¡œ ë¡œë“œë˜ëŠ” ëª¨ë“  ë¶€ì„œ ëª©ë¡ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    all_departments = []
    print("ğŸ¯ 1ë‹¨ê³„: Seleniumìœ¼ë¡œ ì „ì²´ ë¶€ì„œ ëª©ë¡ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    try:
        options = Options()
        options.add_argument("--headless")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36")
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
    except Exception as e:
        print(f"âŒ Selenium ë“œë¼ì´ë²„ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

    try:
        for category, url in base_urls.items():
            print(f"  - [{category}] í˜ì´ì§€ ì ‘ì† ë° ë¶„ì„ ì¤‘...")
            driver.get(url)
            time.sleep(2)
            
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            dept_links = soup.select("a.x_tag")
            
            count = 0
            for link in dept_links:
                href = link.get('href')
                if not href or 'javascript:' in href: continue
                full_url = urljoin(url, href)
                dept_name = link.get('title', '').strip() or link.get_text(strip=True).replace('# ', '')
                if dept_name and dept_name != 'ì „ì²´':
                    all_departments.append({'category': category, 'name': dept_name, 'url': full_url})
                    count += 1
            print(f"    -> {count}ê°œ ë¶€ì„œ ìˆ˜ì§‘ ì™„ë£Œ.")
    except Exception as e:
        print(f"  - í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        driver.quit()
        print("\nâœ… Selenium ë“œë¼ì´ë²„ ì¢…ë£Œ.")
            
    print(f"\nâœ… 1ë‹¨ê³„ ì™„ë£Œ: ì´ {len(all_departments)}ê°œì˜ ë¶€ì„œ ë§í¬(ì¤‘ë³µ í¬í•¨)ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    return all_departments

def fetch_doctors_from_department(department, headers):
    """ì£¼ì–´ì§„ ë¶€ì„œ í˜ì´ì§€ì—ì„œ ëª¨ë“  ì˜ë£Œì§„ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    dept_name = department['name']
    category = department['category']
    url = department['url']
    
    parsed_url = urlparse(url)
    query_params = parse_qs(parsed_url.query)
    dept_no = query_params.get('deptNo', [None])[0]

    doctors = []
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        doctor_items = soup.select("ul.c_doc_list > li.doc_blk")
        for item in doctor_items:
            name_tag = item.select_one("p.tit span.t")
            name = name_tag.get_text(strip=True) if name_tag else "ì´ë¦„ ì •ë³´ ì—†ìŒ"
            
            specialty_tag = item.select_one("dl.txt dd.link")
            specialty = specialty_tag.get_text(strip=True) if specialty_tag else "ì „ë¬¸ë¶„ì•¼ ì •ë³´ ì—†ìŒ"
            
            prof_no = "ID ì—†ìŒ"
            link_tag = item.select_one("div.btn_w a[href*='openDoctorView']")
            if link_tag:
                href_attr = link_tag.get('href', '')
                match = re.search(r"openDoctorView\(\s*'.*?',\s*'([^']*)'\s*\)", href_attr)
                if match:
                    prof_no = match.group(1)

            doctors.append({
                'ì†Œì†ë¶„ë¥˜': category, 'ì†Œì†ë¶€ì„œ': dept_name, 'ì´ë¦„': name,
                'ì „ë¬¸ë¶„ì•¼': specialty, 'deptNo': dept_no, 'profNo': prof_no
            })
        return doctors
    except requests.exceptions.RequestException as e:
        print(f"      - {dept_name} ì˜ë£Œì§„ ì •ë³´ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
        return []

def fetch_doctor_details(doctor, headers):
    """requestsë¡œ íŒì—… HTMLì— ìˆ¨ê²¨ì§„ í•™ë ¥/ê²½ë ¥ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    dept_no = doctor.get('deptNo')
    prof_no = doctor.get('profNo')
    
    if not dept_no or not prof_no or prof_no == "ID ì—†ìŒ":
        return {"í•™ë ¥": "ìƒì„¸ ì •ë³´ ì¡°íšŒ ë¶ˆê°€", "ê²½ë ¥": "ìƒì„¸ ì •ë³´ ì¡°íšŒ ë¶ˆê°€"}
        
    detail_url = f"https://hosp.ajoumc.or.kr/doctor/profViewPop.do?deptNo={dept_no}&profNo={prof_no}"
    details = {"í•™ë ¥": "ì •ë³´ ì—†ìŒ", "ê²½ë ¥": "ì •ë³´ ì—†ìŒ"}
    
    try:
        response = requests.get(detail_url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ìˆ¨ê²¨ì§„ mobileìš© divì—ì„œ ì •ë³´ ì¶”ì¶œ
        career_area = soup.select_one("div#careerMobArea")
        if career_area:
            sections = career_area.select("ul.detailsBox_txt_list > li")
            for section in sections:
                title_tag = section.select_one("p.tit span.t")
                if not title_tag: continue
                
                title = title_tag.get_text(strip=True)
                items = []
                for li in section.select("ul.list_basic.list_dot > li"):
                    # <span> ì•ˆì˜ í…ìŠ¤íŠ¸ë¥¼ ê³µë°±ìœ¼ë¡œ í•©ì³ì„œ ì •ë¦¬
                    item_text = ' '.join(li.find('span').find_all(string=True, recursive=False)).strip()
                    item_text = re.sub(r'\s+', ' ', item_text) # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
                    items.append(item_text)
                
                if items:
                    if 'í•™ë ¥' in title:
                        details['í•™ë ¥'] = "\n".join(items)
                    elif 'ê²½ë ¥' in title:
                        details['ê²½ë ¥'] = "\n".join(items)

    except requests.exceptions.RequestException as e:
        print(f"      [Error] ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬: {e}")
    
    return details

if __name__ == "__main__":
    base_urls = {
        "ì§„ë£Œê³¼": "https://hosp.ajoumc.or.kr/doctor/profDeptList.do",
        "ì „ë¬¸ì„¼í„°": "https://hosp.ajoumc.or.kr/doctor/profCenterList.do",
        "ì•”ì„¼í„°": "https://hosp.ajoumc.or.kr/doctor/profCancerList.do",
        "ì¹˜ê³¼ë³‘ì›": "https://hosp.ajoumc.or.kr/doctor/profDentalList.do",
        "ì „ë¬¸í´ë¦¬ë‹‰": "https://hosp.ajoumc.or.kr/doctor/profClinicList.do"
    }
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36'
    }

    json_output_file = 'ajou_doctors_with_details.json'
    excel_output_file = 'ajou_doctors_with_details.xlsx'
    
    departments = get_all_departments_selenium(base_urls)
    
    if departments:
        unique_checker_dept = {}
        for dept in departments:
            unique_key = (dept['name'], dept['url'])
            if unique_key not in unique_checker_dept:
                unique_checker_dept[unique_key] = dept
        unique_departments = list(unique_checker_dept.values())
        print(f"\nâœ… ì¤‘ë³µ ì œê±° í›„, ìµœì¢… {len(unique_departments)}ê°œì˜ ë¶€ì„œë¥¼ ëŒ€ìƒìœ¼ë¡œ 2ë‹¨ê³„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

        all_doctors = []
        print("\nğŸ¯ 2ë‹¨ê³„: ê° ë¶€ì„œë³„ ì˜ë£Œì§„ ëª©ë¡ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        for i, dept in enumerate(unique_departments):
            print(f"  - ({i+1}/{len(unique_departments)}) {dept['name']} ({dept['category']}) ì˜ë£Œì§„ ìˆ˜ì§‘ ì¤‘...")
            doctors_in_dept = fetch_doctors_from_department(dept, headers)
            if doctors_in_dept:
                print(f"    -> {len(doctors_in_dept)}ëª… ìˆ˜ì§‘ ì™„ë£Œ.")
                all_doctors.extend(doctors_in_dept)
            else:
                print("    -> ì˜ë£Œì§„ ì •ë³´ ì—†ìŒ.")
            time.sleep(0.3)
            
        print(f"\nâœ… 2ë‹¨ê³„ ì™„ë£Œ: ìˆ˜ì§‘ëœ ì˜ë£Œì§„ ì •ë³´ëŠ” ì´ {len(all_doctors)}ê±´ ì…ë‹ˆë‹¤.")
        
        unique_doctors = {doc['profNo']: doc for doc in all_doctors if doc['profNo'] != "ID ì—†ìŒ"}.values()
        print(f"âœ… ì¤‘ë³µ ì œê±° í›„, ìµœì¢… {len(unique_doctors)}ëª…ì˜ ì˜ë£Œì§„ ì •ë³´ë¥¼ ëŒ€ìƒìœ¼ë¡œ 3ë‹¨ê³„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

        print("\nğŸ¯ 3ë‹¨ê³„: ê° ì˜ë£Œì§„ì˜ ìƒì„¸ ì •ë³´(í•™ë ¥/ê²½ë ¥) ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        final_data = []
        for i, doc in enumerate(list(unique_doctors)):
            print(f"  - ({i+1}/{len(unique_doctors)}) {doc['ì´ë¦„']} ì˜ë£Œì§„ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
            details = fetch_doctor_details(doc, headers) # Selenium ë“œë¼ì´ë²„ê°€ ë” ì´ìƒ í•„ìš” ì—†ìŒ
            doc['í•™ë ¥'] = details['í•™ë ¥']
            doc['ê²½ë ¥'] = details['ê²½ë ¥']
            final_data.append(doc)
            time.sleep(0.3)

        print(f"\nâœ… 3ë‹¨ê³„ ì™„ë£Œ: ìµœì¢…ì ìœ¼ë¡œ {len(final_data)}ëª…ì˜ ìƒì„¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        save_to_json(final_data, json_output_file)

        # 2. ì €ì¥ëœ JSON íŒŒì¼ì„ Excel íŒŒì¼ë¡œ ë³€í™˜ (ìƒˆë¡œ ì¶”ê°€ëœ ë¶€ë¶„)
        convert_json_to_excel(json_output_file, excel_output_file)
    else:
        print("\nâŒ 1ë‹¨ê³„ ë¶€ì„œ ìˆ˜ì§‘ì— ì‹¤íŒ¨í•˜ì—¬ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")